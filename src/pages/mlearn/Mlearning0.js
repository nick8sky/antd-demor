import React, { Component } from 'react';

class Mlearning0 extends Component {
    render() {
        return (
            <div>
                <p>单词</p>
                <p>Optimization优化问题</p>
                <p>Gradient 梯度</p>
                <p>Cost </p>
                <p>Global optimal /aptimo/ </p>
                <p>Local optimal</p>
                <p>hypothesis  /haiposeses/  假设</p>
                <p>&nbsp;</p>
                <h3>机器学习有什么理论去支撑它是可行的？</h3>
                <p>把训练集扔到机器学习算法里去学习一下，得到一个模型。在这一步，实际上就是学习得到一个模型h(hypothesis )，这个模型h对训练集里的样本而言误差很小，E_in(h)≈0。“然后就可以用这个模型去判断训练集之外的样本了”，在这一步模型(h)对训练集以外的样本的误差也是很小的。E_out(h)≈E_in(h)。</p>
                <p>机器学习所做的事情就是去想尽各种办法寻找一个h保证这E_in(h)≈0和E_out(h)≈E_in(h)成立，只有这两个式子成立，机器学习所得到的模型才能真正有效果。</p>
                <p><strong>现在确实存在一个模型f，可以对所有的(未知集)做出正确的判断，即E_out(f)=E_in(f)=0</strong>。我们想知道这个模型f是什么样子的，可是我们没有直接的办法去求它，所以机器学习所做的事情是想从假设空间中找到一个很接近模型f的模型h，就可以认为h在一定程度上就可以代替我们真正想寻找的模型f了。如果模型h满足了那上面两个式子，那么就会有E_out(h)≈E_in(h)≈0，所以模型h就可以一定程度上代替模型f。</p>
                <p><strong>E_in可以说成训练误差，经验误差，training error,empirical risk, empirical error。 E_out 被称为泛化误差，生成误差 generalization error。</strong>我们可以发现，E_in并不能真正反映你的模型到底是好是坏，E_out这个误差才能真正地反映模型的预测的准确性。 </p>
                <p>经常可以看到泛化，到底什么是泛化？我们用训练集去训练得到一个模型，然后把这个模型运用到训练集以外的样本上，这个过程就称为泛化。把这个模型运用到训练集以外的样本上的表现的好坏称为泛化能力。模型h的泛化能力强是指：当E_in(h)≈0时，然后把模型运用到训练集以外的样本上所得到的E_out(h)≈0。模型h的泛化能力弱是指：当E_in(h)≈0时，然后把模型运用到训练集以外的样本上所得到的E_out(h)&gt;&gt;0。</p>
                <p>对一个名词的说明：本文中所出现的“模型”一词，在本文中的解释是模型就是某一个h。“机器学习算法产生通过训练得到一个模型”，这句话的意思就是机器学习在假设空间中的众多h中找到一个最好的h，这个最好的h就是机器学习算法得到的模型。h又是什么呢？可以先简单的理解成某一个h是一个固定的函数。例如这个假设空间里面所包含很多不一样的直线，h=wx+b(w,b各不相同)。那么机器学习算法需要做的就是从这么多条直线中挑选出最好的那一条直线(h∗=w∗x+b∗)作为输出模型。</p>
                <p>我们如何去求E_in和 E_out的关系呢？ </p>
                <p>E_in是一个小集合样本里面的错误出现概率。E _out是一个大集合样本里面错误出现的概率。这个小集合和大集合都是服从同一分布的。 原来你肯定遇到过一个这样的概率论问题：让你统计一个非常大的集合里面，某一个事件出现的概率。我们当时是这么做的：从这个大集合里面去随机挑出来一个小集合，然后在这个小集合里面求出这个事件出现的概率，然后我们就可以很高兴的说大集合里面这个事件出现的概率就是这么多了。这个现在来看虽然当时的做法有些不严谨，不过确实是有一些合理性的。 霍夫丁不等式保证了我们的做法的合理的</p>
                <p><img src={require('../../img/20160420140222038.png')}/></p>
                <p>在图片中可以看出，我们用抽样出来的橘色小球的概率去出现估计瓶子里橘色小球的概率。霍夫丁不等式保证了|v−μ|&gt;ε出现的概率有一个上界。 </p>
                <p>可以把这种思想引入到机器学习中。</p>
                <p>&nbsp;</p>
                <h4>假设空间是finite（有限大）的情况</h4>
                <p><em>我们先令假设空间H里面有k个可能的h</em> </p>
                <p>P(∃h∈H.|E_in(h)−E_out(h)|&gt;ε)=P(|E_in(h1)−E_out(h1)|&gt;ε∪ ）U  P(E_in(h2)−E_out(h2) |&gt;ε∪）U...</p>
                <p><strong>A∪B∪C发生的概率是小于每个事件发生概率之和：</strong></p>
                <p><img src={require('../../img/20160420154119974.png')}/></p>
                <p>所以必有：P(∃h∈H.|E_in(h)−E_out(h)|&gt;ε) &lt;=2kexp(−2ε^2N)</p>
                <h4>假设空间是infinite（无限）的情况</h4>
                <p>为什么要引入VC维？当假设空间是有限大的时候，我们就可以用假设空间里面包含的h的个数k去衡量这个算法所产生的模型的复杂程度。现在假设空间的大小是无限大的，我们没有假设空间的大小这种类似的衡量指标了，所以这个时候就需要引入了VC维。
                    VC维：它可以代表这个算法所产生模型的复杂程度。这个算法产生的模型越复杂，那这个算法的dvc就越大。反着，亦然。 </p>
                <p>vc维度后面细讲。</p>
                <p>&nbsp;</p>
                <p><strong>可是问题是E_in(h)≈0这个式子会怎么样啊？</strong>答案是随着模型的复杂度的降低，E_in(h)会越来越大于0。（为什么？最简单解释是：我们如果想去描述一件物品，例如苹果，我们用的描述词越多，我们对这个物品的描述就会越好。当然实际情况稍微有点偏差，随着描述词越来越多，对描述效果的影响会越来越小。这个就像是你用2个词比用1个词对描述效果的提升是巨大的；可是如果你用10个词和你用9个词的话，这时候对描述效果的提升就微乎其微了）。</p>
                <p>我们想要同时满足两个式子，这不是矛盾的么！！！
                    模型复杂度越小，E_out和E_in就越接近，可是E_in就越不接近0。
                    模型复杂度越大，E_in就越接近0，可是E_out和E_in就越不接近。

                    这个矛盾还可以称作偏差(bias) 和 方差(variance)的矛盾，欠拟合(underfitting)和过拟合(overfitting)的矛盾。</p>
                <p>模型复杂度越小，E_out和E_in就越接近，可是E_in就越不接近0，这时variance会变小，<strong>bias会变大</strong>，导致欠拟合(underfitting)的问题。</p>
                <p>模型复杂度越大，E_in(h)就越接近0，可是E_out和E_in就越不接近，这时bias会变小，<strong>variance会变大</strong>，可能导致过拟合(overfitting)的问题。</p>
                <p><img src={require('../../img/20160420194745449.png')}/></p>
                <p>图中的out-of-sample error就是E_out，in-sample error就是E_in。</p>
                <p>1、随着模型复杂度(model complexity)的变大，E_in会变得越来越小，不过减小速度越来越慢，到最后会趋于直线；</p>
                <p>2、E_out是先减小，后变大的（这个减小过程可以理解成：刚开始模型复杂度不是特别大，E_out和E_in的差距很小，随着模型复杂度的增加，E_in的减小幅度非常大，虽然说E_out和E_in的差距会变大，不过这时的决定因素是E_in的减小，所以E_out(h)也会被带着减小。那个增加的过程的理解成：这个时候的模型复杂度已经很大了，随着模型复杂的增加，E_in的减小已经趋向于平缓了，这时E_out和E_in的差距变得越来大。</p>

            </div>
        );
    }
}

export default Mlearning0;